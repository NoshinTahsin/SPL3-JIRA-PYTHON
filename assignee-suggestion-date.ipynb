{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import math\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "#stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "#for word in input_str:\n",
    " #   print(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python library for interacting with JIRA via REST APIs.\n",
    "#https://jira.readthedocs.io/en/latest/\n",
    "from jira import JIRA\n",
    "#connect to the jira sever using HTTP basic auth\n",
    "jira = JIRA(basic_auth=('bsse0914@iit.du.ac.bd', 'MbaYDePZRnDgAdL062r83605'), options={'server':'https://pg-req.atlassian.net'})\n",
    "#The library is able to load the credentials from inside the ~/.netrc file, \n",
    "#so put them there instead of keeping them in your source code.\n",
    "#need to work on it - how to handle authentication\n",
    "#do i need this auth if i am already logged in?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C1R\n",
      "12\n",
      "FAZ\n",
      "5\n",
      "FEP\n",
      "4\n",
      "RMS\n",
      "0\n",
      "SAAJ\n",
      "7\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "projects = jira.projects()\n",
    "num_of_projects=len(projects)\n",
    "issueCount=0\n",
    "for i in range(0,num_of_projects):\n",
    "    print(projects[i])\n",
    "    #jql\n",
    "    issues_in_proj = jira.search_issues('project='+str(projects[i]))\n",
    "    print(len(issues_in_proj))\n",
    "    issueCount+=len(issues_in_proj)\n",
    "print(issueCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string \n",
    "def remove_punctuation(text):\n",
    "    no_punc = \"\".join([c for c in text if c not in string.punctuation])\n",
    "    return no_punc.lower()\n",
    "\n",
    "def tokenize(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    porter_stemmer= PorterStemmer()\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    terms_list = []\n",
    "    #stemmed_sentence = []\n",
    "\n",
    "    #Stemming is a little more aggressive. It cuts off prefixes and/or endings of words\n",
    "    #based on common ones. It can sometimes be helpful, but not always because often times \n",
    "    #the new word is so much a root that it loses its actual meaning. \n",
    "    #Lemmatizing, on the other hand, maps common words into one base. \n",
    "    #Unlike stemming though, it always still returns a proper word that can be found in the dictionary.\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            #w=porter_stemmer.stem(w)\n",
    "            w=lemmatizer.lemmatize(w)\n",
    "            terms_list.append(w)\n",
    "\n",
    "    #print(word_tokens)\n",
    "    #print(\"terms list\")\n",
    "    #print(terms_list)\n",
    "    \n",
    "    return terms_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final map:\n",
      "{'Moumita Asad': ['write', 'icse', 'paper', 'including', 'finding', 'regression', 'analysis', 'primary', 'demonstration', 'write', 'blog', 'productivity', 'employee', 'wfh', 'publish', 'dsse', 'website', 'explore', 'literature', 'find', 'paper', 'related', 'employee', 'productivity', 'work', 'home'], 'Noshin Tahsin': ['regression', 'analysis', 'data', 'write', 'paper', 'finding', 'far', 'arxiv', 'create', 'pie', 'chart', 'data', 'visualization', 'clean', 'preprocess', 'data', 'change', 'column', 'name', 'drop', 'invalid', 'encode', 'categorical', 'find', 'productivityrelated', 'factor', 'related', 'paper', 'prepare', 'survey', 'questionnare', 'getting', 'list', 'view', 'queried', 'result', 'user', 'vscode', 'extension', 'exploring', 'feature', 'better', 'strategy', 'viewing', 'implementing', 'search', 'viewer', '404', 'error', 'need', 'solve', 'khay', 'oneksolve', 'kor', 'assignee', 'suggestion', 'table', 'add', 'image', 'along', 'name', 'make', 'ui', 'attractive', 'change', 'design', 'ui', 'jira', 'addon', 'using', 'ace', 'framework', 'guideline', 'component', 'found', 'atlassian'], 'Mridha Md. Nafis Fuad': ['create', 'bar', 'chart', 'stackedbars', 'visualizing', 'data', 'visualization', 'setting', 'encoder', 'decoder', 'attention', 'mechanism', 'train', 'model', 'get', 'context', 'vector', 'pyhton', 'neural', 'structure', 'parse', 'ast', 'code', 'snippet', 'sbt', 'way', 'python', 'parsing', 'write', 'tool', 'paper', 'quickly', 'methodology', 'done'], 'Toukir Ahammed': ['conduct', 'survey', 'collect', 'data', 'convey', 'pilot', 'survey', 'industry', 'professional', 'incorporate', 'feedback', 'change', 'questionnaire', 'accordingly'], 'A. T. M. Fazlay Rabbi': ['getting', 'exception', 'unknown', 'token', 'test', 'input', 'java', 'method', 'fixing', 'tokenizer', 'deploy', 'backend', 'heroku', 'getting', '404', 'form', 'input', 'provided', 'issue', 'heroku', 'fix', 'backend', 'deploy', 'backend', 'heroku']}\n",
      "Account ID map:\n",
      "{'Moumita Asad': '5f5764e4bea5be0068101b9a', 'Noshin Tahsin': '5e299526bf04010e70c42927', 'Mridha Md. Nafis Fuad': '5ae1622f027e7a2ebfcc12d1', 'Toukir Ahammed': '5ca99ba2e929a772795e4ac9', 'A. T. M. Fazlay Rabbi': '5e9412627bc0680c2ca4dd39'}\n"
     ]
    }
   ],
   "source": [
    "cnt=0\n",
    "assignee_terms_map={}\n",
    "terms_assignee_map={}\n",
    "terms_assignee_list=[]\n",
    "account_id_map={}\n",
    "\n",
    "for p in projects:\n",
    "    pro_key='project='\n",
    "    pro_key+=str(p.key)\n",
    "    project_key=pro_key\n",
    "    size = 100\n",
    "    initial = 0\n",
    "    issueCount=0\n",
    "    while True:\n",
    "        start= initial*size\n",
    "        issues = jira.search_issues(project_key,  start,size)\n",
    "        issueCount+=len(issues)\n",
    "        if len(issues) == 0:\n",
    "            break\n",
    "        initial += 1\n",
    "        for issue in issues:\n",
    "            if issue.fields.assignee and issue.fields.assignee!=None:\n",
    "                cnt+=1\n",
    "                summary = issue.fields.summary\n",
    "                des = issue.fields.description\n",
    "                assignee = str(issue.fields.assignee)\n",
    "                account_id = str(issue.fields.assignee.accountId)\n",
    "                #dateofissue\n",
    "                #issuetype\n",
    "                #done #todo #inprogress\n",
    "                #assignee - assignee,date (dict)\n",
    "                \n",
    "                des=remove_punctuation(des)\n",
    " \n",
    "                bad_chars=['!','@', '#', '$','%', '^', '&','*','(',')','-','+']\n",
    "                des = ''.join(i for i in des if not i in bad_chars)\n",
    "                tokenized_des=tokenize(des)\n",
    "        \n",
    "                summary=remove_punctuation(summary)\n",
    " \n",
    "                bad_chars=['!','@', '#', '$','%', '^', '&','*','(',')','-','+']\n",
    "                summary = ''.join(i for i in summary if not i in bad_chars)\n",
    "                tokenized_summary=tokenize(summary)\n",
    "               \n",
    "                #list of all terms existing in an issue\n",
    "                dupli_terms=tokenized_des+tokenized_summary\n",
    "            \n",
    "                terms = list(dict.fromkeys(dupli_terms))\n",
    "                #terms_dict=term:assignee.time\n",
    "                \n",
    "                keylist=[]\n",
    "                for key,val in assignee_terms_map.items():\n",
    "                    keylist.append(key)\n",
    "                                \n",
    "                if assignee in keylist:\n",
    "                    assignee_terms_map[assignee]=assignee_terms_map[assignee]+terms\n",
    "                else:\n",
    "                    assignee_terms_map[assignee]=terms\n",
    "                    #assignee_terms_map[assignee.dev_name]=terms_dict\n",
    "\n",
    "                    account_id_map[assignee]=account_id\n",
    "    \n",
    "                terms_assignee_list.append(assignee)\n",
    "                #append assignee date\n",
    "                #assignee is a dict -dev,date\n",
    "            \n",
    "                for term in terms:\n",
    "                    if term in terms_assignee_map:\n",
    "                        terms_assignee_map[term]=terms_assignee_map[term]+terms_assignee_list\n",
    "                    else:\n",
    "                        terms_assignee_map[term]=terms_assignee_list  \n",
    "                    \n",
    "                    #terms_assignee_date_map\n",
    "                    \n",
    "                terms_assignee_list=[]\n",
    "\n",
    "print(\"Final map:\")\n",
    "print(assignee_terms_map)\n",
    "\n",
    "print(\"Account ID map:\")\n",
    "print(account_id_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(0,num_of_projects):\n",
    " #   all_issues=jira.search_issues('project='+str(projects[i]))\n",
    "  #  num_of_issues=len(all_issues)\n",
    "   # for j in range(0,num_of_issues):\n",
    "        #if i!=0 or j!=14:\n",
    "        #issue = data[\"projects\"][i]['issues'][j]\n",
    "     #   issue = all_issues[j]   \n",
    "    #    \n",
    "#print(assignee_terms_map)\n",
    "#print(terms_assignee_map)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "['Moumita Asad', 'Noshin Tahsin', 'Mridha Md. Nafis Fuad', 'Toukir Ahammed', 'A. T. M. Fazlay Rabbi']\n",
      "Moumita Asad => ['write', 'icse', 'paper', 'including', 'finding', 'regression', 'analysis', 'primary', 'demonstration', 'write', 'blog', 'productivity', 'employee', 'wfh', 'publish', 'dsse', 'website', 'explore', 'literature', 'find', 'paper', 'related', 'employee', 'productivity', 'work', 'home']\n",
      "Noshin Tahsin => ['regression', 'analysis', 'data', 'write', 'paper', 'finding', 'far', 'arxiv', 'create', 'pie', 'chart', 'data', 'visualization', 'clean', 'preprocess', 'data', 'change', 'column', 'name', 'drop', 'invalid', 'encode', 'categorical', 'find', 'productivityrelated', 'factor', 'related', 'paper', 'prepare', 'survey', 'questionnare', 'getting', 'list', 'view', 'queried', 'result', 'user', 'vscode', 'extension', 'exploring', 'feature', 'better', 'strategy', 'viewing', 'implementing', 'search', 'viewer', '404', 'error', 'need', 'solve', 'khay', 'oneksolve', 'kor', 'assignee', 'suggestion', 'table', 'add', 'image', 'along', 'name', 'make', 'ui', 'attractive', 'change', 'design', 'ui', 'jira', 'addon', 'using', 'ace', 'framework', 'guideline', 'component', 'found', 'atlassian']\n",
      "Mridha Md. Nafis Fuad => ['create', 'bar', 'chart', 'stackedbars', 'visualizing', 'data', 'visualization', 'setting', 'encoder', 'decoder', 'attention', 'mechanism', 'train', 'model', 'get', 'context', 'vector', 'pyhton', 'neural', 'structure', 'parse', 'ast', 'code', 'snippet', 'sbt', 'way', 'python', 'parsing', 'write', 'tool', 'paper', 'quickly', 'methodology', 'done']\n",
      "Toukir Ahammed => ['conduct', 'survey', 'collect', 'data', 'convey', 'pilot', 'survey', 'industry', 'professional', 'incorporate', 'feedback', 'change', 'questionnaire', 'accordingly']\n",
      "A. T. M. Fazlay Rabbi => ['getting', 'exception', 'unknown', 'token', 'test', 'input', 'java', 'method', 'fixing', 'tokenizer', 'getting', '404', 'form', 'input', 'provided', 'issue', 'heroku', 'fix', 'backend', 'deploy', 'backend', 'heroku']\n"
     ]
    }
   ],
   "source": [
    "print(len(assignee_terms_map))\n",
    "available_devlist=[]\n",
    "\n",
    "for key,val in assignee_terms_map.items():\n",
    "    available_devlist.append(key)\n",
    "print(available_devlist) \n",
    "#del assignee_terms_map[\"None\"]\n",
    "for key,val in  assignee_terms_map.items():\n",
    "    print(key, \"=>\", val)\n",
    "    #print(key)\n",
    "    #print(len(val))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139\n",
      "{'write': ['Moumita Asad', 'Noshin Tahsin', 'Moumita Asad', 'Mridha Md. Nafis Fuad'], 'icse': ['Moumita Asad'], 'paper': ['Moumita Asad', 'Noshin Tahsin', 'Noshin Tahsin', 'Moumita Asad', 'Mridha Md. Nafis Fuad'], 'including': ['Moumita Asad'], 'finding': ['Moumita Asad', 'Noshin Tahsin'], 'regression': ['Moumita Asad', 'Noshin Tahsin'], 'analysis': ['Moumita Asad', 'Noshin Tahsin'], 'data': ['Noshin Tahsin', 'Noshin Tahsin', 'Mridha Md. Nafis Fuad', 'Noshin Tahsin', 'Toukir Ahammed'], 'far': ['Noshin Tahsin'], 'arxiv': ['Noshin Tahsin'], 'primary': ['Moumita Asad'], 'demonstration': ['Moumita Asad'], 'blog': ['Moumita Asad'], 'productivity': ['Moumita Asad', 'Moumita Asad'], 'employee': ['Moumita Asad', 'Moumita Asad'], 'wfh': ['Moumita Asad'], 'publish': ['Moumita Asad'], 'dsse': ['Moumita Asad'], 'website': ['Moumita Asad'], 'create': ['Noshin Tahsin', 'Mridha Md. Nafis Fuad'], 'pie': ['Noshin Tahsin'], 'chart': ['Noshin Tahsin', 'Mridha Md. Nafis Fuad'], 'visualization': ['Noshin Tahsin', 'Mridha Md. Nafis Fuad'], 'bar': ['Mridha Md. Nafis Fuad'], 'stackedbars': ['Mridha Md. Nafis Fuad'], 'visualizing': ['Mridha Md. Nafis Fuad'], 'clean': ['Noshin Tahsin'], 'preprocess': ['Noshin Tahsin'], 'change': ['Noshin Tahsin', 'Toukir Ahammed', 'Noshin Tahsin'], 'column': ['Noshin Tahsin'], 'name': ['Noshin Tahsin', 'Noshin Tahsin'], 'drop': ['Noshin Tahsin'], 'invalid': ['Noshin Tahsin'], 'encode': ['Noshin Tahsin'], 'categorical': ['Noshin Tahsin'], 'conduct': ['Toukir Ahammed'], 'survey': ['Toukir Ahammed', 'Toukir Ahammed', 'Noshin Tahsin'], 'collect': ['Toukir Ahammed'], 'convey': ['Toukir Ahammed'], 'pilot': ['Toukir Ahammed'], 'industry': ['Toukir Ahammed'], 'professional': ['Toukir Ahammed'], 'incorporate': ['Toukir Ahammed'], 'feedback': ['Toukir Ahammed'], 'questionnaire': ['Toukir Ahammed'], 'accordingly': ['Toukir Ahammed'], 'find': ['Noshin Tahsin', 'Moumita Asad'], 'productivityrelated': ['Noshin Tahsin'], 'factor': ['Noshin Tahsin'], 'related': ['Noshin Tahsin', 'Moumita Asad'], 'prepare': ['Noshin Tahsin'], 'questionnare': ['Noshin Tahsin'], 'explore': ['Moumita Asad'], 'literature': ['Moumita Asad'], 'work': ['Moumita Asad'], 'home': ['Moumita Asad'], 'getting': ['Noshin Tahsin', 'A. T. M. Fazlay Rabbi', 'A. T. M. Fazlay Rabbi'], 'list': ['Noshin Tahsin'], 'view': ['Noshin Tahsin'], 'queried': ['Noshin Tahsin'], 'result': ['Noshin Tahsin'], 'user': ['Noshin Tahsin'], 'vscode': ['Noshin Tahsin'], 'extension': ['Noshin Tahsin'], 'exploring': ['Noshin Tahsin'], 'feature': ['Noshin Tahsin'], 'better': ['Noshin Tahsin'], 'strategy': ['Noshin Tahsin'], 'viewing': ['Noshin Tahsin'], 'implementing': ['Noshin Tahsin'], 'search': ['Noshin Tahsin'], 'viewer': ['Noshin Tahsin'], 'exception': ['A. T. M. Fazlay Rabbi'], 'unknown': ['A. T. M. Fazlay Rabbi'], 'token': ['A. T. M. Fazlay Rabbi'], 'test': ['A. T. M. Fazlay Rabbi'], 'input': ['A. T. M. Fazlay Rabbi', 'A. T. M. Fazlay Rabbi'], 'java': ['A. T. M. Fazlay Rabbi'], 'method': ['A. T. M. Fazlay Rabbi'], 'fixing': ['A. T. M. Fazlay Rabbi'], 'tokenizer': ['A. T. M. Fazlay Rabbi'], 'setting': ['Mridha Md. Nafis Fuad'], 'encoder': ['Mridha Md. Nafis Fuad'], 'decoder': ['Mridha Md. Nafis Fuad'], 'attention': ['Mridha Md. Nafis Fuad'], 'mechanism': ['Mridha Md. Nafis Fuad'], 'train': ['Mridha Md. Nafis Fuad'], 'model': ['Mridha Md. Nafis Fuad'], 'get': ['Mridha Md. Nafis Fuad'], 'context': ['Mridha Md. Nafis Fuad'], 'vector': ['Mridha Md. Nafis Fuad'], 'pyhton': ['Mridha Md. Nafis Fuad'], 'neural': ['Mridha Md. Nafis Fuad'], 'structure': ['Mridha Md. Nafis Fuad'], 'parse': ['Mridha Md. Nafis Fuad'], 'ast': ['Mridha Md. Nafis Fuad'], 'code': ['Mridha Md. Nafis Fuad'], 'snippet': ['Mridha Md. Nafis Fuad'], 'sbt': ['Mridha Md. Nafis Fuad'], 'way': ['Mridha Md. Nafis Fuad'], 'python': ['Mridha Md. Nafis Fuad'], 'parsing': ['Mridha Md. Nafis Fuad'], 'tool': ['Mridha Md. Nafis Fuad'], 'quickly': ['Mridha Md. Nafis Fuad'], 'methodology': ['Mridha Md. Nafis Fuad'], 'done': ['Mridha Md. Nafis Fuad'], '404': ['Noshin Tahsin', 'A. T. M. Fazlay Rabbi'], 'error': ['Noshin Tahsin'], 'need': ['Noshin Tahsin'], 'solve': ['Noshin Tahsin'], 'khay': ['Noshin Tahsin'], 'oneksolve': ['Noshin Tahsin'], 'kor': ['Noshin Tahsin'], 'form': ['A. T. M. Fazlay Rabbi'], 'provided': ['A. T. M. Fazlay Rabbi'], 'issue': ['A. T. M. Fazlay Rabbi'], 'heroku': ['A. T. M. Fazlay Rabbi', 'A. T. M. Fazlay Rabbi'], 'fix': ['A. T. M. Fazlay Rabbi'], 'backend': ['A. T. M. Fazlay Rabbi', 'A. T. M. Fazlay Rabbi'], 'deploy': ['A. T. M. Fazlay Rabbi'], 'assignee': ['Noshin Tahsin'], 'suggestion': ['Noshin Tahsin'], 'table': ['Noshin Tahsin'], 'add': ['Noshin Tahsin'], 'image': ['Noshin Tahsin'], 'along': ['Noshin Tahsin'], 'make': ['Noshin Tahsin'], 'ui': ['Noshin Tahsin', 'Noshin Tahsin'], 'attractive': ['Noshin Tahsin'], 'design': ['Noshin Tahsin'], 'jira': ['Noshin Tahsin'], 'addon': ['Noshin Tahsin'], 'using': ['Noshin Tahsin'], 'ace': ['Noshin Tahsin'], 'framework': ['Noshin Tahsin'], 'guideline': ['Noshin Tahsin'], 'component': ['Noshin Tahsin'], 'found': ['Noshin Tahsin'], 'atlassian': ['Noshin Tahsin']}\n"
     ]
    }
   ],
   "source": [
    "#now i have mapped the assignee ans terms\n",
    "#now need to map term => dev\n",
    "print(len(terms_assignee_map))\n",
    "add=0\n",
    "#for key,val in  terms_assignee_map.items():\n",
    "    #print(key, \"=>\", val)\n",
    "    #print(len(val))\n",
    "    #add=add+len(val)\n",
    "    #print(add)\n",
    "    \n",
    "#1347 unique terms??\n",
    "\n",
    "#did nothing for sparse terms - that occur less frequently\n",
    "#nltk.download('wordnet')\n",
    "#print(len(terms_assignee_map['paper']))\n",
    "#for i in range (0,len(terms_assignee_map['paper'])):\n",
    "    #print(terms_assignee_map['paper'][i])\n",
    "print(terms_assignee_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['explore', 'literature', 'find', 'tool', 'paper', 'icse', 'related', 'regression', 'analysis', 'related', 'data', 'find', 'tool', 'paper', 'icse']\n"
     ]
    }
   ],
   "source": [
    "#tokenizing the new issue\n",
    "#new_des=data[\"projects\"][0]['issues'][14]['description']\n",
    "\n",
    "#all_new_issues=jira.search_issues('project='+str(projects[0]))\n",
    "#new_issue=all_new_issues[0]\n",
    "\n",
    "new_issue=jira.issue('C1R-12')\n",
    "new_des=new_issue.fields.description\n",
    "\n",
    "new_des=remove_punctuation(new_des)\n",
    "bad_chars=['!','@', '#', '$','%', '^', '&','*','(',')','-','+']\n",
    "new_des = ''.join(i for i in new_des if not i in bad_chars)\n",
    "tokenized_new_des=tokenize(new_des)\n",
    "\n",
    "#new_summary=data[\"projects\"][0]['issues'][14]['description']\n",
    "#new_date\n",
    "\n",
    "new_summary=new_issue.fields.summary\n",
    "\n",
    "new_summary=remove_punctuation(new_summary)\n",
    "bad_chars=['!','@', '#', '$','%', '^', '&','*','(',')','-','+']\n",
    "new_summary = ''.join(i for i in new_summary if not i in bad_chars)\n",
    "tokenized_new_summary=tokenize(new_summary)\n",
    "\n",
    "new_terms=tokenized_new_des+tokenized_new_summary\n",
    "print(new_terms)\n",
    "\n",
    "#now i have the new issue terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explore\n",
      "['Moumita Asad']\n",
      "literature\n",
      "['Moumita Asad']\n",
      "find\n",
      "['Noshin Tahsin', 'Moumita Asad']\n",
      "tool\n",
      "['Mridha Md. Nafis Fuad']\n",
      "paper\n",
      "['Moumita Asad', 'Noshin Tahsin', 'Noshin Tahsin', 'Moumita Asad', 'Mridha Md. Nafis Fuad']\n",
      "icse\n",
      "['Moumita Asad']\n",
      "related\n",
      "['Noshin Tahsin', 'Moumita Asad']\n",
      "regression\n",
      "['Moumita Asad', 'Noshin Tahsin']\n",
      "analysis\n",
      "['Moumita Asad', 'Noshin Tahsin']\n",
      "related\n",
      "['Noshin Tahsin', 'Moumita Asad']\n",
      "data\n",
      "['Noshin Tahsin', 'Noshin Tahsin', 'Mridha Md. Nafis Fuad', 'Noshin Tahsin', 'Toukir Ahammed']\n",
      "find\n",
      "['Noshin Tahsin', 'Moumita Asad']\n",
      "tool\n",
      "['Mridha Md. Nafis Fuad']\n",
      "paper\n",
      "['Moumita Asad', 'Noshin Tahsin', 'Noshin Tahsin', 'Moumita Asad', 'Mridha Md. Nafis Fuad']\n",
      "icse\n",
      "['Moumita Asad']\n"
     ]
    }
   ],
   "source": [
    "#now calculate score for developer suggestion\n",
    "#print(len(new_terms))\n",
    "#term - dev_dict{dev: ,date:}\n",
    "newterm_dev_map={}\n",
    "#def getDevTermUsageInfo():\n",
    "cnt=0\n",
    "for newterm in new_terms:\n",
    "    if newterm in terms_assignee_map:\n",
    "        #print(cnt)\n",
    "        cnt=cnt+1\n",
    "        print(newterm)\n",
    "        print(terms_assignee_map[newterm])\n",
    "        newterm_dev_map[newterm]=terms_assignee_map[newterm]\n",
    "        #returns a devlist for a single newterm\n",
    "        #developers who has previously worked on that term\n",
    "        #one dev name can occur multiple times\n",
    "        #which implies he has worked on that term multiple times-more experienced!\n",
    "\n",
    "#print(newterm_dev_map['everything'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_termInfo={}\n",
    "\n",
    "for dev in available_devlist:\n",
    "    #print(dev)\n",
    "    term_info={}\n",
    "    #check if he has used a newterm and get the newterm\n",
    "    for key in newterm_dev_map:\n",
    "        if dev in newterm_dev_map[key]:\n",
    "            #now i need the key-key is the newterm\n",
    "            #calculate term info for the key\n",
    "            #just need the fixfrequency\n",
    "            #how many times dev is in newterm_dev_map[key]\n",
    "            devnames=newterm_dev_map[key]\n",
    "            #print(devnames)\n",
    "            dev_count=devnames.count(dev)\n",
    "            #print(key)\n",
    "            #print(dev_count)\n",
    "            #List-devcount,termusedate\n",
    "            term_info[key]=dev_count #add term_use_date : last fixing time of a term by a developer\n",
    "            \n",
    "    dev_termInfo[dev]=term_info\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moumita Asad  =>  {'explore': 1, 'literature': 1, 'find': 1, 'paper': 2, 'icse': 1, 'related': 1, 'regression': 1, 'analysis': 1}\n",
      "Noshin Tahsin  =>  {'find': 1, 'paper': 2, 'related': 1, 'regression': 1, 'analysis': 1, 'data': 3}\n",
      "Mridha Md. Nafis Fuad  =>  {'tool': 1, 'paper': 1, 'data': 1}\n",
      "Toukir Ahammed  =>  {'data': 1}\n",
      "A. T. M. Fazlay Rabbi  =>  {}\n"
     ]
    }
   ],
   "source": [
    "for key,value in dev_termInfo.items():\n",
    "    print(key,\" => \",value)\n",
    "#Toukir Ahammed  =>  {'data': 2, (date last worked)}\n",
    "#now i have the complex map of dev term term-info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Moumita Asad': 9.515127912330904, 'Noshin Tahsin': 6.219291046326574, 'Mridha Md. Nafis Fuad': 2.6310891599660815, 'Toukir Ahammed': 0.5108256237659907, 'A. T. M. Fazlay Rabbi': 0}\n"
     ]
    }
   ],
   "source": [
    "prediction_list=[]\n",
    "\n",
    "def devFixFreq(term):\n",
    "    dev_use_count=0\n",
    "    for dev in assignee_terms_map:\n",
    "        terms_used=assignee_terms_map[dev]\n",
    "        if term in terms_used:#if term in terms used keylist\n",
    "            dev_use_count=dev_use_count+1\n",
    "    return dev_use_count\n",
    "    \n",
    "dev_score={} \n",
    "for key in dev_termInfo:\n",
    "    expertise=0\n",
    "    termInfo=dev_termInfo[key]\n",
    "    #print(termInfo)\n",
    "    for info_term in termInfo:\n",
    "        term_freq=termInfo[info_term]\n",
    "        #print(info_term)\n",
    "        #print(term_freq)\n",
    "        ##dev=number of devs in the project \n",
    "        #len(available_devlist)\n",
    "        \n",
    "        #devfixfreq() : returns numof devs who fixed the term related bugs\n",
    "        num_of_devs=len(available_devlist)\n",
    "        tfIdf=term_freq*math.log(num_of_devs/devFixFreq(info_term))\n",
    "        #What about time data?\n",
    "        expertise+=tfIdf\n",
    "    #end of for\n",
    "    dev_score[key]=expertise\n",
    "#end of for\n",
    "print(dev_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moumita Asad  :: 9.515127912330904\n",
      "Noshin Tahsin  :: 6.219291046326574\n",
      "Mridha Md. Nafis Fuad  :: 2.6310891599660815\n",
      "Toukir Ahammed  :: 0.5108256237659907\n",
      "A. T. M. Fazlay Rabbi  :: 0\n"
     ]
    }
   ],
   "source": [
    "sorted_dev_score = sorted(dev_score.items() , reverse=True, key=lambda x: x[1])\n",
    "# Iterate over the sorted sequence\n",
    "for elem in sorted_dev_score :\n",
    "    print(elem[0] , \" ::\" , elem[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moumita Asad\n",
      "Account ID\n",
      "5f5764e4bea5be0068101b9a\n",
      "\n",
      "\n",
      "Noshin Tahsin\n",
      "Account ID\n",
      "5e299526bf04010e70c42927\n",
      "\n",
      "\n",
      "Mridha Md. Nafis Fuad\n",
      "Account ID\n",
      "5ae1622f027e7a2ebfcc12d1\n",
      "\n",
      "\n",
      "Toukir Ahammed\n",
      "Account ID\n",
      "5ca99ba2e929a772795e4ac9\n",
      "\n",
      "\n",
      "A. T. M. Fazlay Rabbi\n",
      "Account ID\n",
      "5e9412627bc0680c2ca4dd39\n",
      "\n",
      "\n",
      "['Moumita Asad', 'Noshin Tahsin', 'Mridha Md. Nafis Fuad', 'Toukir Ahammed', 'A. T. M. Fazlay Rabbi']\n",
      "['5f5764e4bea5be0068101b9a', '5e299526bf04010e70c42927', '5ae1622f027e7a2ebfcc12d1', '5ca99ba2e929a772795e4ac9', '5e9412627bc0680c2ca4dd39']\n"
     ]
    }
   ],
   "source": [
    "a=[]\n",
    "a_id=[]\n",
    "for i in range (0,len(sorted_dev_score)):\n",
    "    a.append(sorted_dev_score[i][0])\n",
    "    a_id.append(account_id_map[sorted_dev_score[i][0]])\n",
    "    print(sorted_dev_score[i][0])\n",
    "    print(\"Account ID\")\n",
    "    print(account_id_map[sorted_dev_score[i][0]])\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(a)\n",
    "print(a_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
